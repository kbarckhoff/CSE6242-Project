pip install google-cloud-storage

# =========================
# CONFIG
# =========================
USE_GCS = True  # set to False to read from LOCAL_CSV
PROJECT_ID = "cse6242-team-project"
BUCKET_NAME = "zillow_raw_data"
BLOB_NAME = "Rental_zip_raw_data.csv"   # exact name in your bucket

LOCAL_CSV = r"C:\path\to\Rental_zip_raw_data.csv"  # used if USE_GCS=False
OUTPUT_PRED_CSV = "predicted_volatility_by_zip.csv"  # written to CWD

# =========================
# IMPORTS
# =========================
import io
import math
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd

# light, dependable tree model that trains quickly on tabular data
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# =========================
# LOAD DATA
# =========================
def load_rent_df(use_gcs=True):
    if use_gcs:
        from google.cloud import storage
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(BUCKET_NAME)
        blob = bucket.blob(BLOB_NAME)
        data = blob.download_as_bytes()
        df = pd.read_csv(io.BytesIO(data))
    else:
        df = pd.read_csv(LOCAL_CSV)
    return df

raw = load_rent_df(USE_GCS)
print("Loaded shape:", raw.shape)
print("Sample columns:", list(raw.columns[:12]))

# =========================
# CLEAN / PARSE (WIDE -> LONG)
# =========================
# Normalize column names for safety
raw = raw.rename(columns={
    "RegionName": "zip",
    "State": "State",
    "City": "City",
    "Metro": "Metro",
    "CountyName": "CountyName"
})

# Some Zillow exports include 'StateName' or duplicate geo fields; keep the essentials if present
geo_cols = [c for c in ["zip","State","City","Metro","CountyName"] if c in raw.columns]
# Identify all monthly columns like '2023-10-31'
date_cols = [c for c in raw.columns if len(c) >= 7 and c[:4].isdigit() and "-" in c]

# Keep only geo + time columns
df_wide = raw[geo_cols + date_cols].copy()

# Melt to long
long = df_wide.melt(
    id_vars=geo_cols,
    value_vars=date_cols,
    var_name="date",
    value_name="rent_usd"
)
long["date"] = pd.to_datetime(long["date"], errors="coerce")
long = long.dropna(subset=["date", "rent_usd"])
# Enforce ZIP as 5-char string (leading zeros)
long["zip"] = long["zip"].astype(str).str.zfill(5)

# Sort for time operations
long = long.sort_values(["zip", "date"]).reset_index(drop=True)

# Keep only realistic rents
long = long[(long["rent_usd"] > 100) & (long["rent_usd"] < 10000)].copy()

print("Long shape:", long.shape)
print(long.head())

# =========================
# FEATURE ENGINEERING
# =========================
# Monthly return (percentage change) — do not forward-fill; keep gaps
long["ret_m"] = (
    long.groupby("zip", dropna=False)["rent_usd"]
        .pct_change(fill_method=None)
)

# Trailing features (computed on historical data only)
def add_trailing_features(g):
    g = g.copy()
    # rolling vols (std of returns) in % units
    g["vol_3m"]  = g["ret_m"].rolling(3, min_periods=2).std()  * 100.0
    g["vol_6m"]  = g["ret_m"].rolling(6, min_periods=3).std()  * 100.0
    g["vol_12m"] = g["ret_m"].rolling(12, min_periods=4).std() * 100.0
    # recent momentum (% change last 3m and 6m)
    g["mom_3m"]  = g["rent_usd"].pct_change(3, fill_method=None)  * 100.0
    g["mom_6m"]  = g["rent_usd"].pct_change(6, fill_method=None)  * 100.0
    # last month’s change
    g["ret_1m"]  = g["ret_m"] * 100.0
    return g

long = long.groupby("zip", group_keys=False).apply(add_trailing_features)

# Calendar month (seasonality)
long["month"] = long["date"].dt.month

# =========================
# TARGETS: FUTURE VOLATILITY (1,3,6,12m)
# =========================
HORIZONS = [3, 6, 12]  # months ahead
for H in HORIZONS:
    # future rolling std of *future* returns over next H months
    col = f"fut_vol_{H}m"
    def future_vol(g):
        g = g.copy()
        # shift(-1) to start returns from next month, then rolling H
        fv = (
            g["ret_m"].shift(-1).rolling(H, min_periods=max(2, min(H, 3))).std() * 100.0
        )
        return fv
    long[col] = long.groupby("zip", group_keys=False).apply(future_vol)

# Keep rows with at least one target present
target_cols = [f"fut_vol_{h}m" for h in HORIZONS]
long_any_target = long.dropna(subset=target_cols, how="all")

# =========================
# TRAIN / EVALUATE per HORIZON
# =========================
# Time-based split: last 3 calendar months = test (if present)
latest_date = long_any_target["date"].max()
test_cutoff = (latest_date - pd.offsets.MonthEnd(3)).normalize() + pd.offsets.MonthEnd(0)

# Features to use
feature_cols = [
    "rent_usd", "ret_1m", "vol_3m", "vol_6m", "vol_12m", "mom_3m", "mom_6m", "month"
]

rows_train_total = 0
rows_test_total  = 0
metrics = []

# We’ll collect predictions at the *latest* date per ZIP for each horizon
latest_per_zip = (
    long.groupby("zip", as_index=False)
        .tail(1)[["zip","date","State","City","Metro","CountyName","rent_usd"]]
        .rename(columns={"rent_usd":"latest_rent"})
)

pred_df = latest_per_zip.copy()

for H in HORIZONS:
    y_col = f"fut_vol_{H}m"
    use = long_any_target.dropna(subset=[y_col]).copy()

    # Ensure we have the features
    use = use.dropna(subset=feature_cols)

    if use.empty:
        print(f"[{H}m] Skipping: no rows with features + target.")
        continue

    train = use[use["date"] <= test_cutoff]
    test  = use[use["date"] >  test_cutoff]

    rows_train_total += len(train)
    rows_test_total  += len(test)

    if len(train) == 0 or len(test) == 0:
        print(f"[{H}m] Not enough temporal coverage to create a train/test split. Training on all.")
        train = use
        test  = use.iloc[0:0]  # empty test

    X_tr, y_tr = train[feature_cols], train[y_col]
    X_te, y_te = test[feature_cols],  test[y_col]

    model = GradientBoostingRegressor(random_state=42)
    model.fit(X_tr, y_tr)

    # Evaluate on test (if available)
    if len(test) > 0:
        y_hat = model.predict(X_te)
        mae = mean_absolute_error(y_te, y_hat)
        r2  = r2_score(y_te, y_hat)
        metrics.append({"horizon_m": H, "rows_train": len(train), "rows_test": len(test),
                        "MAE_vol(%)": mae, "R2": r2})
    else:
        metrics.append({"horizon_m": H, "rows_train": len(train), "rows_test": 0,
                        "MAE_vol(%)": np.nan, "R2": np.nan})

    # Predict at the latest row per ZIP
    latest_rows = (
        long.sort_values("date")
            .groupby("zip", as_index=False)
            .tail(1)
    )
    latest_rows = latest_rows.copy()
    latest_rows = latest_rows.dropna(subset=feature_cols)
    if latest_rows.empty:
        pred_df[f"pred_vol_next_{H}m_pct"] = np.nan
        continue

    preds = model.predict(latest_rows[feature_cols])

    pred_map = latest_rows[["zip"]].copy()
    pred_map[f"pred_vol_next_{H}m_pct"] = preds

    pred_df = pred_df.merge(pred_map, on="zip", how="left")

# =========================
# OUTPUTS
# =========================
metrics_df = pd.DataFrame(metrics).sort_values("horizon_m")
print("\nEvaluation (test set):")
print(metrics_df.to_string(index=False))

# Keep nice order
keep_cols = ["zip","date","State","City","Metro","CountyName","latest_rent"] + \
            [c for c in pred_df.columns if c.startswith("pred_vol_next_")]
pred_df = pred_df[keep_cols].sort_values(["State","City","zip"])

pred_df.to_csv(OUTPUT_PRED_CSV, index=False)
print(f"\n✅ Saved: {OUTPUT_PRED_CSV}")

# Show a few rows
print("\nSample predictions:")
print(pred_df.head(10).to_string(index=False))
